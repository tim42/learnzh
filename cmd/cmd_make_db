#!/bin/bash # just for automatic syntactic coloration purpose

function _create_conf_file # file
{
    local file="$1"
    log "generating config file: $file"
    log_cmd rm -f "$file"

    echo -n >"$file"
    cat <<EOF >>"$file"
##
##  configuration file
##  generated on $(has date && date || echo -n '[unknown]')
##

##
## CONF VARS
##

EOF
    var_list="$(compgen -A variable "conf_")"
    for var in $var_list
    do
      help_var="help_$var"
      echo "# ${!help_var}" >>"$file"
      echo "$var='${!var}'" >>"$file"
      echo >>"$file"
    done

    cat <<EOF >>"$file"

##
## GLOBAL OPTIONS
##

EOF

    var_list="$(compgen -A variable "gbl_opt_")"
    for var in $var_list
    do
      var_exists "noconf_$var" && continue

      help_var="help_$var"
      echo "# ${!help_var}" >>"$file"
      echo "$var='${!var}'" >>"$file"
      echo >>"$file"
    done

    cat <<EOF >>"$file"

##
## COMMAND OPTIONS
##

EOF
    var_list="$(compgen -A variable "cmd_opt_")"
    for var in $var_list
    do
      var_exists "noconf_$var" && continue

      help_var="help_$var"
      echo "# ${!help_var}" >>"$file"
      echo "$var='${!var}'" >>"$file"
      echo >>"$file"
    done
}

function _reset_word_db # raw_dict_file dest_language_code
{
    local raw_dict_file="$1"
    local language_code="$2"

    local line_count=$(grep -vE '^#' <"$raw_dict_file" | wc -l)
    log "Resetting the $language_code DB (source file contains $line_count entries)"

    # the destination format is: |trad-word|zhuyin|/(descriptions/)+
    # but to speed things up we go to the intermediate trad-word|/(descriptions/)+
    # and add a slash at the start of each lines
    #
    # NOTE: I do not take the trad word from the dict as I do not trust the source of that trad word as it can be either be HK-trad or TW-trad.
    #       so instead a conversion from simplified to TW-trad is done to have a more consistent content.
    (
        paste -d '|||\n' \
           <(grep -vE '^#' "$raw_dict_file" | cut -d' ' -f2 | opencc -c s2tw.json ) \
           <(grep -vE '^#' "$raw_dict_file" | perl -p -e 's/.*?\[(.*?)\].*/\1/g' | tr '[:upper:]' '[:lower:]' | pinyin_to_zhuyin ) \
           <(grep -vE '^#' "$raw_dict_file" | perl -p -e 's@.*?[/](.*)[/]@/$1/@g') #\
    ) | sed -e 's/^/|/g'  >"${conf_db_file}_${language_code}"
}

function _reset_sentence_db # raw_sentence_file
{
    # NOTE: the source file has some badly-space-separated pinyin and will sometimes produce incorrect sentences.
    local raw_sentence_file="$1"

    # this is a words splitter from pinyin. It is super slow, but necessary.

    # here are the initials and the finals of any pinyin word. We use those to build a regex that would match any phoneme.
    # (building the regex by hand would be too error prone and I dislike debugging regex
    local pinyin_initials=(zh ch sh b p m f d t n l g k h j q x r z c s y w)
    local pinyin_finals=(iang iong uang  ang  eng  ing iao ian  ong  uai uan   an ao ai   en er ei  ia iu ie in  ou un ua uo ue ui  a e i o u v )

    # build the regex.
    # initials are not required, but finals are.
    # (some finals can form words. not all of them, but we assume the input data is correct so having a filter matching incorrect data is not a problem).
    # building the regex is slower than having is precomputed, but compared to the slowness of the rest of this code _it's fine_.
    local initials_regex=
    local it=
    for it in "${pinyin_initials[@]}"
    do
        initials_regex="$initials_regex|$it"
    done
    initials_regex="(${initials_regex:1})"

    local finals_regex=
    for it in "${pinyin_finals[@]}"
    do
        finals_regex="$finals_regex|$it"
    done
    finals_regex="(${finals_regex:1})"

    # the regex !
    regex="${initials_regex}?${finals_regex}"

    local line=
    local line_count=$(grep -vE '^#' <"$raw_sentence_file" | wc -l)
    local i=0

    log "Resetting the sentence (source file contains $line_count entries)"

    # fast way: (faster 'cause single invocation + single compilation of regexes)
    # The first <(...) do:
    #  - read the pinyin section of the file
    #  - remove any accent (ü -> u), remove "'", convert to lower case
    #  - in the perl:
    #    - open the source file, read a line, remove punctuation, spaces, digit and non-chinese words
    #    - for each pinyin character, replace it with the corresponding chinese character from the source file. This gives the list of words, space separated.
    paste >"$conf_sentence_db_file" -d'||\n' \
        <(cut -d'	' -f2 < "$raw_sentence_file" | iconv -f utf8 -t ascii//TRANSLIT | tr "'" ' ' | tr '[:upper:]' '[:lower:]' |
            perl -e "use open ':std', ':encoding(UTF-8)';use utf8;" \
                 -e "BEGIN{open(RAW, '<', '$raw_sentence_file') or die \$!;}" \
                 -p \
                 -e 'my $line = <RAW>;'\
                 -e '$line=~s/[[:punct:]]|[[:space:]]|[0-9a-zA-Z]|[｀]//g;'\
                 -e 'my $cont=0;' \
                 -e "s/${regex}/substr(\$line,\$cont++,1)/eg;") \
        <(cut -d'	' -f1 < "$raw_sentence_file") \
        <(cut -d'	' -f3 < "$raw_sentence_file")
}

function browser_curl # url
{
  debug_log "executing curl -s $1"
  curl -s "$1" -H 'User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:84.0) Gecko/20100101 Firefox/84.0' -H 'Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8' -H 'Accept-Language: en-US,en;q=0.5' --compressed -H 'DNT: 1' -H 'Connection: keep-alive' -H 'Upgrade-Insecure-Requests: 1'
}

function _download_and_gunzip # url
{
  browser_curl "$1" | gunzip -c
}

function _download_and_unzip # url file
{
  local file="$(mktemp -p "$conf_base_path/" "tmp_XXXX.zip")"
  browser_curl "$1" > "$file"
  unzip -p "$file" "$2"
  rm "$file"
}


# dictionary must follow the same pattern as cc-cedict
function _download_and_unzip_dict_if_not_present # language_code url [dl_function, [options]]
{
  local language_code="$1"
  local url="$2"
  local dl_fucntion="${3:-_download_and_gunzip}"
  shift 3
  local raw_dict_file="$conf_base_path/zh_${language_code}_raw_dict.txt"

  if [ ! -e "$raw_dict_file" ]
  then
      log "Downloading the raw $language_code dict file as it does not exist..."
      $dl_fucntion "$url" "$@" > "$raw_dict_file"
  fi
  _reset_word_db "$raw_dict_file" "$language_code"
}

function ext_cmd_make_db
{
    local raw_en_dict_file="$conf_base_path/cedict_1_0_ts_utf-8_mdbg.txt"
    local raw_sentence_file="$conf_base_path/sentence_raw_data.txt"

    # make sure stuff exists, even if empty:
    mkdir -p "$conf_base_path"
    mkdir -p "$conf_base_path/data"
    mkdir -p "$conf_base_path/upload"

    # touch, not remove (so... you don't end-up paying a high price a mistake)
    touch "$conf_test_db_file"
    touch "$conf_test_db_file_lock"

    cache_init "$conf_wiktionary_cache"
    cache_init "$conf_wiktionary_zh_cache"
    cache_init "$conf_tts_cache"

    [ ! -f "$main_rc_file" ] && _create_conf_file "$main_rc_file"

    # make sure the source files exist + reset the db files
    _download_and_unzip_dict_if_not_present en https://www.mdbg.net/chinese/export/cedict/cedict_1_0_ts_utf-8_mdbg.txt.gz
    _download_and_unzip_dict_if_not_present fr https://chine.in/mandarin/dictionnaire/CFDICT/cfdict.zip _download_and_unzip "cfdict.u8"
    if [ ! -e "$raw_sentence_file" ]
    then
        log "Downloading the raw sentence list file as it does not exist..."
        # super shady link, pointing to a forum resource and all...
        # Skip the first line as it is not content
        browser_curl https://www.plecoforums.com/download/18-896-hsk-sentences-ascending-complexity-txt.2413/ | tail -n+2 | opencc -c s2tw.json > "$raw_sentence_file"
    fi

    # download and strip the word frequency file if it does not exists:
    if [ ! -e "$conf_word_freq_list_file" ]
    then
        log "Downloading the word frequency file as it does not exist..."
        _download_and_gunzip "https://www.ugent.be/pp/experimentele-psychologie/en/research/documents/subtlexch/subtlexch131210.zip" | tail -n+2 | cut -d'	' -f1 > "$conf_word_freq_list_file"
    fi

    # clear the db file & reformat the raw db file:
    _reset_sentence_db "$raw_sentence_file"

    log done.
}

